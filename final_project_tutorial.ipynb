{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44afbef3",
   "metadata": {},
   "source": [
    "# Final Project Starting Guide\n",
    "\n",
    "Hello everyone, welcome to the final project! This notebook is provided to you to reiterate the rules and guidelines, and give you some starting points.\n",
    "\n",
    "### What we provide\n",
    "\n",
    "In this project, we will provide you with \n",
    "- This starting guide\n",
    "- A working API that you can access under ASU network (i.e., on campus or with VPN)\n",
    "- A starting development data that you can use to develop your agent. It contains 1,000 instances with {domain, input, expected_output}\n",
    "\n",
    "### Your goal\n",
    "\n",
    "In this project, you will implement an inference-time agent to solve reasoning requests, as those provided in the development data. The grading of this project will be effort-based and you will get full credit if you produce the minimum deliverables below, with subject to the rules and requirements below.\n",
    "\n",
    "#### Minimum Deliverables\n",
    "\n",
    "1. A working agent loop (in the form of a Github project) that the TA can run, and implements *at least three* inference-time algorithms or techniques.\n",
    "2. Outputs from your agent on the released test data (see important dates). \n",
    "3. A short one-page report on how your agent works, and pointer to important techniques (referece to code blocks).\n",
    "\n",
    "#### Rules and Requirements\n",
    "1. You must only use our provided API call to access LLMs; meaning that you cannot use any other LLMs in any other way within your agent loop. Some exceptions may be made if you call certain external tools (e.g., Google search) that use some LLMs internally. Please discuss any exceptions with us to avoid penalties up to 50% of the project grade.\n",
    "2. You must not hardcode a full delegation to an external tool (e.g., google_search(input_problem)). Such delegations must be automatically selected/decided by your agent. Hardcode delegations will lead to a zero.\n",
    "3. You cannot use Cursor or any AI coding aids to implement the final project. You can, however, ask LLMs (or other online resources) for conceptual clarification or code examples. Your final project should not contain any blocks of code (i.e., > 3 lines) that are written by AI. Violations will lead to a zero.\n",
    "4. Your agent should be able to run efficiently, with <20 LLM calls per question. Exceptions may be made when you have a complicated agent but please discuss with us. Up to 10% of the project grade may be deducted if we observe very inefficient LLM usages that do not clearly benefit the performance.\n",
    "5. Your agent must run without any requests to any paid services (paid is defined by if the TA has to pay to run it, regardless of whether you actuallly pay for it or not.) Violations will lead to a zero.\n",
    "6. You must submit a Github project link as your code submission. All changes must be tracked and any commits should be within 100 lines of +/- with good messages. Points will be deducted to up to 25% of the project grade if we observe \"magic commits\" or too few commits. \n",
    "\n",
    "\n",
    "### Suggestions\n",
    "1. Start early, please.\n",
    "2. You should consider how you can evaluate whether your output is good enough compared to the provided expected_outputs, and we will not release how we will actually evaluate your outputs; meaning that you have to try to predict how we will evaluate things.\n",
    "3. Start with a basic implementation, and iterate based on mistakes/feedbacks.\n",
    "4. Find more development data, or create your own cases to stree-test your agent. \n",
    "5. You are free to modify any provided code in this starting guide, or not using any of these code at all.\n",
    "\n",
    "### Important dates\n",
    "- **Release of final test data**: 11/25/2025\n",
    "- **Deadline for submitting all deliverables**: 12/05/2025\n",
    "\n",
    "### Extra Credit. \n",
    "The top 20 projects (ranked by performance metrics on the test data and at the TA's discretion of implementation quality) will be given extra credits. The actual credits will be between 1% to 7.5% depending on the ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7858ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Minimal setup\n",
    "# If needed (uncomment in a notebook):\n",
    "# !pip install requests python-dotenv\n",
    "\n",
    "import os, json, textwrap, re, time\n",
    "from dotenv import load_dotenv\n",
    "from ddgs import DDGS \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "import requests\n",
    "\n",
    "# API_KEY  = os.getenv(\"OPENAI_API_KEY\", \"cse476\")\n",
    "# API_BASE = os.getenv(\"API_BASE\", \"http://10.4.58.53:41701/v1\")  \n",
    "# MODEL    = os.getenv(\"MODEL_NAME\", \"bens_model\")  \n",
    "\n",
    "API_KEY=\"cse476\"\n",
    "API_BASE=\"http://10.4.58.53:41701/v1\"\n",
    "MODEL=\"bens_model\"            \n",
    "\n",
    "def call_model_chat_completions(prompt: str,\n",
    "                                system: str = \"give me only the final answer no explanation.\",\n",
    "                                model: str = MODEL,\n",
    "                                temperature: float = 0.0,\n",
    "                                timeout: int = 60,\n",
    "                                max_tokens: int = 128,\n",
    "                                message: list = []) -> dict:\n",
    "    \"\"\"\n",
    "    Calls an OpenAI-style /v1/chat/completions endpoint and returns:\n",
    "    { 'ok': bool, 'text': str or None, 'raw': dict or None, 'status': int, 'error': str or None, 'headers': dict }\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\":  \"application/json\",\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"messages\": message,\n",
    "        \"temperature\": temperature,\n",
    "        # \"response_format\":{\"type\":\"text\", \"strict\":False},#added this to force text output\n",
    "        \"max_tokens\": max_tokens,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        resp = requests.post(url, headers=headers, json=payload, timeout=timeout)\n",
    "        status = resp.status_code\n",
    "        hdrs   = dict(resp.headers)\n",
    "        if status == 200:\n",
    "            data = resp.json()\n",
    "            text = data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            return {\"ok\": True, \"text\": text, \"raw\": data, \"status\": status, \"error\": None, \"headers\": hdrs}\n",
    "        else:\n",
    "            # try best-effort to surface error text\n",
    "            err_text = None\n",
    "            try:\n",
    "                err_text = resp.json()\n",
    "            except Exception:\n",
    "                err_text = resp.text\n",
    "            return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": status, \"error\": str(err_text), \"headers\": hdrs}\n",
    "    except requests.RequestException as e:\n",
    "        return {\"ok\": False, \"text\": None, \"raw\": None, \"status\": -1, \"error\": str(e), \"headers\": {}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6362f9",
   "metadata": {},
   "source": [
    "## 1) Smoke test: direct inference\n",
    "\n",
    "We’ll do a single request with a strict instruction to answer briefly.  \n",
    "*If you see an auth error, set `OPENAI_API_KEY` and (if needed) `API_BASE`/`MODEL_NAME`.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8c02ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: False HTTP: 400\n",
      "MODEL SAYS: \n"
     ]
    }
   ],
   "source": [
    "# %% Direct call example\n",
    "demo_prompt = \"What is 17 + 28? Answer with just the number.\"\n",
    "result = call_model_chat_completions(demo_prompt)\n",
    "print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "print(\"MODEL SAYS:\", (result[\"text\"] or \"\").strip())\n",
    "\n",
    "# Optional: Inspect rate-limit headers if your provider exposes them\n",
    "for k in [\"x-ratelimit-remaining-requests\", \"x-ratelimit-limit-requests\", \"x-request-id\"]:\n",
    "    if k in result[\"headers\"]:\n",
    "        print(f\"{k}: {result['headers'][k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae1c568",
   "metadata": {},
   "source": [
    "## 2) A tiny test set (3 questions)\n",
    "\n",
    "We’ll cover:\n",
    "1. **Math reasoning** — inequality solving,\n",
    "2. **Common sense** — buoyancy/ice & water,\n",
    "3. **Logic** — a classic race-position puzzle.\n",
    "\n",
    "We also tightly constrain the required answer forms to enable simple auto‑grading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0334e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Define three tests: input + expected\n",
    "tests = [\n",
    "    {\n",
    "        \"id\": \"math_inequality\",\n",
    "        \"type\": \"numeric\",  # grader will prefer numeric extraction\n",
    "        \"prompt\": \"Solve for the smallest integer n such that 3n + 5 > 26. Answer with just the integer.\",\n",
    "        \"expected\": \"8\",    # Because 3n > 21 => n > 7, smallest integer is 8\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"commonsense_ice\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"You place an ice cube in a glass of water and mark the water level. \"\n",
    "            \"After the ice melts, does the water level rise, fall, or stay the same? \"\n",
    "            \"Answer with exactly one of: 'rise', 'fall', 'stay the same'.\"\n",
    "        ),\n",
    "        \"expected\": \"stay the same\",\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"logic_race\",\n",
    "        \"type\": \"text\",\n",
    "        \"prompt\": (\n",
    "            \"In a race, you pass the person in second place. What position are you now in? \"\n",
    "            \"Answer with a single word like 'first', 'second', 'third'.\"\n",
    "        ),\n",
    "        \"expected\": \"second\",\n",
    "    },\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9840398",
   "metadata": {},
   "source": [
    "## 3) Minimal evaluator\n",
    "\n",
    "We provide some example code to decide whether the agent outputs match the expected outputs, just to give you an idea of how evaluations can be done. You are free to use this code, or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffddeb84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0/3 correct\n",
      "❌ math_inequality: expected='8', got='' (HTTP 400)\n",
      "   error: {'error': {'message': 'list index out of range list index out of range', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n",
      "❌ commonsense_ice: expected='stay the same', got='' (HTTP 400)\n",
      "   error: {'error': {'message': 'list index out of range list index out of range', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n",
      "❌ logic_race: expected='second', got='' (HTTP 400)\n",
      "   error: {'error': {'message': 'list index out of range list index out of range', 'type': 'BadRequestError', 'param': None, 'code': 400}}\n"
     ]
    }
   ],
   "source": [
    "# %% Simple normalization and evaluation helpers\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = (s or \"\").strip().lower()\n",
    "    # Remove surrounding punctuation and extra whitespace\n",
    "    s = re.sub(r\"[^\\w\\s\\-']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    # Map common synonyms used in these tests\n",
    "    synonyms = {\n",
    "        \"unchanged\": \"stay the same\",\n",
    "        \"no change\": \"stay the same\",\n",
    "        \"same\": \"stay the same\",\n",
    "        \"second place\": \"second\",\n",
    "        \"2nd\": \"second\",\n",
    "        \"first place\": \"first\",\n",
    "        \"third place\": \"third\",\n",
    "    }\n",
    "    return synonyms.get(s, s)\n",
    "\n",
    "def extract_number(s: str):\n",
    "    # Returns first number occurrence as string if found, else None\n",
    "    if not s:\n",
    "        return None\n",
    "    m = re.search(r\"[-+]?\\d+(\\.\\d+)?\", s)\n",
    "    return m.group(0) if m else None\n",
    "\n",
    "def grade(expected: str, got: str, kind: str) -> bool:\n",
    "    if kind == \"numeric\":\n",
    "        exp_num = extract_number(expected)\n",
    "        got_num = extract_number(got)\n",
    "        return (exp_num is not None) and (got_num == exp_num)\n",
    "    else:\n",
    "        return normalize_text(got) == normalize_text(expected)\n",
    "\n",
    "def evaluate_tests(tests, model=MODEL):\n",
    "    rows = []\n",
    "    for t in tests:\n",
    "        r = call_model_chat_completions(\n",
    "            t[\"prompt\"],\n",
    "            system=\"You are a careful solver. Reply ONLY with the final answer, nothing else.\",\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        got = (r[\"text\"] or \"\").strip()\n",
    "        is_correct = grade(t[\"expected\"], got, t[\"type\"])\n",
    "        rows.append({\n",
    "            \"id\": t[\"id\"],\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": is_correct,\n",
    "            \"status\": r[\"status\"],\n",
    "            \"error\": r[\"error\"],\n",
    "        })\n",
    "        # Tiny pacing to be polite to the API\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    # Print a small report\n",
    "    correct = sum(1 for x in rows if x[\"correct\"])\n",
    "    print(f\"Score: {correct}/{len(rows)} correct\")\n",
    "    for x in rows:\n",
    "        mark = \"✅\" if x[\"correct\"] else \"❌\"\n",
    "        print(f\"{mark} {x['id']}: expected={x['expected']!r}, got={x['got']!r} (HTTP {x['status']})\")\n",
    "        if x[\"error\"]:\n",
    "            print(\"   error:\", x[\"error\"])\n",
    "    return rows\n",
    "\n",
    "results = evaluate_tests(tests)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6559aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_evaluate(question, prediction, expected_answer, model=MODEL):\n",
    "    \"\"\"\n",
    "    Use the model itself as a strict grader.\n",
    "    Returns True if the model says the prediction matches the expected answer; else False.\n",
    "    Falls back to a simple normalized string compare if the model's reply is malformed.\n",
    "    \"\"\"\n",
    "    import re\n",
    "\n",
    "    system = \"You are a strict grader. Reply with exactly True or False. No punctuation. No explanation.\"\n",
    "    prompt = f\"\"\"You are grading a question-answer pair.\n",
    "\n",
    "Return exactly True if the PREDICTION would be accepted as correct for the EXPECTED_ANSWER.\n",
    "Otherwise, return False.\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "PREDICTION:\n",
    "{prediction}\n",
    "\n",
    "EXPECTED_ANSWER:\n",
    "{expected_answer}\n",
    "\n",
    "Answer with exactly: True or False\n",
    "\"\"\"\n",
    "\n",
    "    r = call_model_chat_completions(\n",
    "        prompt,\n",
    "        system=system,\n",
    "        model=model,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    reply = (r.get(\"text\") or \"\").strip().lower()\n",
    "    if reply.startswith(\"true\"):\n",
    "        return True\n",
    "    if reply.startswith(\"false\"):\n",
    "        return False\n",
    "\n",
    "    # Fallback: simple normalization-based equality\n",
    "    norm = lambda s: re.sub(r\"\\s+\", \" \", (s or \"\").strip().lower())\n",
    "    return norm(prediction) == norm(expected_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7bfad26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ math_inequality: expected='8', got='4' (HTTP 200)\n",
      "✅ commonsense_ice: expected='stay the same', got='stay the same' (HTTP 200)\n",
      "✅ logic_race: expected='second', got='second' (HTTP 200)\n"
     ]
    }
   ],
   "source": [
    "def self_evaluate_tests(tests, model=MODEL, grader_model=None, sleep_sec=0.2, verbose=True):\n",
    "    \"\"\"\n",
    "    Run the tests by querying the model for each prompt, then use LLM-as-a-judge\n",
    "    (self_evaluate) to determine correctness.\n",
    "\n",
    "    Args:\n",
    "        tests: list of dicts with keys: id, prompt, expected (and optionally type)\n",
    "        model: model used to generate predictions\n",
    "        grader_model: model used to judge correctness (defaults to `model` if None)\n",
    "        sleep_sec: small delay between calls to be polite to the API\n",
    "        verbose: if True, print a summary line per test\n",
    "\n",
    "    Returns:\n",
    "        rows: list of dicts with fields:\n",
    "              id, expected, got, correct, status, error\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    judge_model = grader_model or model\n",
    "    rows = []\n",
    "\n",
    "    for t in tests:\n",
    "        # 1) Get model prediction\n",
    "        r = call_model_chat_completions(\n",
    "            t[\"prompt\"],\n",
    "            system=\"You are a careful solver. Reply ONLY with the final answer, nothing else.\",\n",
    "            model=model,\n",
    "            temperature=0.0,\n",
    "        )\n",
    "        got = (r.get(\"text\") or \"\").strip()\n",
    "\n",
    "        # 2) LLM-as-a-judge: strict True/False\n",
    "        is_correct = self_evaluate(\n",
    "            question=t[\"prompt\"],\n",
    "            prediction=got,\n",
    "            expected_answer=t[\"expected\"],\n",
    "            model=judge_model,\n",
    "        )\n",
    "\n",
    "        row = {\n",
    "            \"id\": t.get(\"id\", \"<unnamed>\"),\n",
    "            \"expected\": t[\"expected\"],\n",
    "            \"got\": got,\n",
    "            \"correct\": bool(is_correct),\n",
    "            \"status\": r.get(\"status\"),\n",
    "            \"error\": r.get(\"error\"),\n",
    "        }\n",
    "        rows.append(row)\n",
    "\n",
    "        if verbose:\n",
    "            mark = \"✅\" if is_correct else \"❌\"\n",
    "            print(f\"{mark} {row['id']}: expected={row['expected']!r}, got={row['got']!r} (HTTP {row['status']})\")\n",
    "            if row[\"error\"]:\n",
    "                print(\"   error:\", row[\"error\"])\n",
    "\n",
    "        if sleep_sec:\n",
    "            time.sleep(sleep_sec)\n",
    "\n",
    "    return rows\n",
    "\n",
    "# Example:\n",
    "results_llm_judge = self_evaluate_tests(tests, verbose=True, model=MODEL, grader_model=MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2424e995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the planning stage for the agent loop\n",
    "#first strategy i'll use is batching - batch multple questions for speed (improve speed)\n",
    "#2nd strategry self refine: ask the agent to critique its own answers and re-answer if wrong (improve accuracy)\n",
    "#3rd use analogical self prompting: asking the model to first self generate similar examples and then answer the question (increase accuracy)\n",
    "    #eg what is the square root of 45 - ask the model to generate similar examples and then answer the question\n",
    "#use test time scalling tts to trigger longer chain of thought improving accuracy. (do this in the self evaluate step add the keyword wait)\n",
    "#improve accuracy set a max token limit \n",
    "#do parallel calls to imrprove speed\n",
    "#multi-llm debatting maybe: if you create multple instances of the model and have them debate each other on the answer to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c21f572",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools for the agent to use:\n",
    "#google search tool\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "\n",
    "def google_search(query:str) -> str:\n",
    "    print('model using google search tool ', query)\n",
    "    try:\n",
    "        with DDGS() as ddgs:\n",
    "                for r in ddgs.text(query, max_results=5): #getting the top 5 reults\n",
    "                    print('google search result ', r.get('body', 'result not found'))\n",
    "                    return r.get('body', 'result not found')\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "#python tool \n",
    "def python_executioner(code):\n",
    "    print('model using python tool ', code)\n",
    "    output_buffer = io.StringIO()\n",
    "    try:\n",
    "        with redirect_stdout(output_buffer):\n",
    "            exec(code, {})\n",
    "        return output_buffer.getvalue()\n",
    "    except Exception as e:\n",
    "        return str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7e32d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 dev examples.\n"
     ]
    }
   ],
   "source": [
    "#reading json dev data\n",
    "try:\n",
    "    with open('cse476_final_project_dev_data.json', 'r') as f:\n",
    "        dev_data = json.load(f)\n",
    "    print(f\"Loaded {len(dev_data)} dev examples.\")\n",
    "except Exception as e:\n",
    "    print('an error has occured loading the file')\n",
    "    dev_data = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac596340",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\achuw\\AppData\\Local\\Temp\\ipykernel_26752\\2378691725.py:1: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  result = call_model_chat_completions(\"Let $ABCD$ be a convex quadrilateral with $AB = CD = 10$ , $BC = 14$ , and $AD = 2\\sqrt{65}$ . Assume that the diagonals of $ABCD$ intersect at point $P$ , and that the sum of the areas of triangles $APB$ and $CPD$ equals the sum of the areas of triangles $BPC$ and $APD$ . Find the area of quadrilateral $ABCD$. Respond with only the final answer. Do not include any explanation or reasoning.\", model=MODEL, temperature=0.0, timeout=60, system=\"give me only the final answer no explanation.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n",
      "OK: True HTTP: 200\n",
      "here is the models response: $ \\boxed{140} $\n"
     ]
    }
   ],
   "source": [
    "result = call_model_chat_completions(\"Let $ABCD$ be a convex quadrilateral with $AB = CD = 10$ , $BC = 14$ , and $AD = 2\\sqrt{65}$ . Assume that the diagonals of $ABCD$ intersect at point $P$ , and that the sum of the areas of triangles $APB$ and $CPD$ equals the sum of the areas of triangles $BPC$ and $APD$ . Find the area of quadrilateral $ABCD$. Respond with only the final answer. Do not include any explanation or reasoning.\", model=MODEL, temperature=0.0, timeout=60, system=\"give me only the final answer no explanation.\")\n",
    "print(\"OK:\", result[\"ok\"], \"HTTP:\", result[\"status\"])\n",
    "print('here is the models response:', result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7ad4c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clearning up the models answer\n",
    "def clean_ans(res):\n",
    "   match = re.search(r'<answer>(.*?)</answer>', res, re.DOTALL | re.IGNORECASE)\n",
    "   if match:\n",
    "        return match.group(1).strip()\n",
    "   return res #return response if  model forgot the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f16f953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What happens if you prick your finger on a spindle?\n",
      "What is the best answer for the question among these?\n",
      "0) If you prick your finger on a spindle, your finger may bleed\n",
      "1) If you prick your finger on a spindle, you will fall into a coma\n",
      "2) If you prick your finger on a spindle, you will be cursed\n",
      "3) If you prick your finger on a spindle, you will be lightly injured\n",
      "4) If you prick your finger on a spindle, you will be lightly injured\n",
      "5) If you prick your finger on a spindle, your finger will sting\n",
      "6) If you prick your finger on a spindle, you will fall asleep\n",
      "7) If you prick your finger on a spindle, you will sleep for a hundred years\n",
      "8) If you prick your finger on a spindle, you will die\n",
      "9) If you prick your finger on a spindle, you may bleed slightly\n",
      "\n",
      "Model Answer with reasoning steps: ### Step 1: Generate a Similar Example Question and Answer\n",
      "\n",
      "**Example Question:**  \n",
      "What happens if you touch a hot stove?\n",
      "\n",
      "**Example Answer:**  \n",
      "If you touch a hot stove, you may get burned.\n",
      "\n",
      "### Step 2: Use the Example to Help Answer the Main Question\n",
      "\n",
      "The example question and answer help us understand that the correct answer to the main question should be based on real-world consequences, not on myths, curses, or exaggerated outcomes. In the example, the answer is grounded in physical reality (getting burned), not in supernatural or exaggerated effects.\n",
      "\n",
      "### Step 3: Analyze the Main Question\n",
      "\n",
      "The main question is:  \n",
      "**\"What happens if you prick your finger on a spindle?\"**\n",
      "\n",
      "A **spindle** is a pointed object, often used in spinning or weaving. If you prick your finger on a spindle, it is likely to cause a small injury, such as a **cut or a prick**, which may result in **bleeding** or **a sting**.\n",
      "\n",
      "### Step 4: Evaluate the Options\n",
      "\n",
      "Let's evaluate the options based on real-world consequences:\n",
      "\n",
      "- **Option 0:** \"If you prick your finger on a spindle, your finger may bleed\"  \n",
      "  ✅ This is a realistic outcome. A spindle is sharp, and a prick can cause bleeding.\n",
      "\n",
      "- **Option 1:** \"If you prick your finger on a spindle, you will fall into a coma\"  \n",
      "  ❌ This is not realistic. A spindle is not strong enough to cause a coma.\n",
      "\n",
      "- **Option 2:** \"If you prick your finger on a spindle, you will be cursed\"  \n",
      "  ❌ This is a myth or superstition, not a real-world consequence.\n",
      "\n",
      "- **Option 3:** \"If you prick your finger on a spindle, you will be lightly injured\"  \n",
      "  ✅ This is a realistic outcome. A prick from a spindle is a minor injury.\n",
      "\n",
      "- **Option 4:** \"If you prick your finger on a spindle, you will be lightly injured\"  \n",
      "  ✅ This is the same as Option 3, so it is also correct.\n",
      "\n",
      "- **Option 5:** \"If you prick your finger on a spindle, your finger will sting\"  \n",
      "  ✅ This is a realistic outcome. A prick can cause a sting.\n",
      "\n",
      "- **Option 6:** \"If you prick your finger on a spindle, you will fall asleep\"  \n",
      "  ❌ This is not realistic. A spindle is not strong enough to cause sleep.\n",
      "\n",
      "- **Option 7:** \"If you prick your finger on a spindle, you will sleep for a hundred years\"  \n",
      "  ❌ This is a myth or superstition, not a real-world consequence.\n",
      "\n",
      "- **Option 8:** \"If you prick your finger on a spindle, you will die\"  \n",
      "  ❌ This is not realistic. A spindle is not strong enough to cause death.\n",
      "\n",
      "- **Option 9:** \"If you prick your finger on a spindle, you may bleed slightly\"  \n",
      "  ✅ This is a realistic outcome. A small prick can cause slight bleeding.\n",
      "\n",
      "### Step 5: Choose the Best Answer\n",
      "\n",
      "Among the options, **Option 0** and **Option 9** are both realistic and accurate. However, **Option 0** is more general and covers the possibility of bleeding, which is a common outcome of a prick.\n",
      "\n",
      "### Final Answer:\n",
      "**Option 0: If you prick your finger on a spindle, your finger may bleed**\n",
      "Model Answer: <answer>Option 0: If you prick your finger on a spindle, your finger may bleed</answer>\n",
      "Expected Answer: If you prick your finger on a spindle, you will be lightly injured\n",
      "\n",
      "Question 2: Mrs. Snyder used to spend 40% of her monthly income on rent and utilities. Her salary was recently increased by $600 so now her rent and utilities only amount to 25% of her monthly income. How much was her previous monthly income?\n",
      "\n",
      "Model Answer with reasoning steps: Step 1: Generate a similar example question along with its answer.\n",
      "\n",
      "Example Question: Mr. Johnson used to spend 30% of his monthly income on groceries. His salary was recently increased by $400 so now his groceries only amount to 20% of his monthly income. How much was his previous monthly income?\n",
      "\n",
      "Example Answer: Let's denote Mr. Johnson's previous monthly income as $ x $. According to the problem, he used to spend 30% of his income on groceries, which means he spent $ 0.30x $ on groceries. After his salary increased by $400, his new monthly income is $ x + 400 $, and his groceries now amount to 20% of his new income, which means he spends $ 0.20(x + 400) $ on groceries. Since the amount spent on groceries remains the same, we can set up the equation:\n",
      "\n",
      "$$ 0.30x = 0.20(x + 400) $$\n",
      "\n",
      "Solving this equation:\n",
      "\n",
      "$$ 0.30x = 0.20x + 80 $$\n",
      "$$ 0.30x - 0.20x = 80 $$\n",
      "$$ 0.10x = 80 $$\n",
      "$$ x = 800 $$\n",
      "\n",
      "So, Mr. Johnson's previous monthly income was $800.\n",
      "\n",
      "Step 2: Use this example to help answer the main question.\n",
      "\n",
      "Now, let's apply the same reasoning to the main question.\n",
      "\n",
      "Main Question: Mrs. Snyder used to spend 40% of her monthly income on rent and utilities. Her salary was recently increased by $600 so now her rent and utilities only amount to 25% of her monthly income. How much was her previous monthly income?\n",
      "\n",
      "Let's denote Mrs. Snyder's previous monthly income as $ y $. According to the problem, she used to spend 40% of her income on rent and utilities, which means she spent $ 0.40y $ on rent and utilities. After her salary increased by $600, her new monthly income is $ y + 600 $, and her rent and utilities now amount to 25% of her new income, which means she spends $ 0.25(y + 600) $ on rent and utilities. Since the amount spent on rent and utilities remains the same, we can set up the equation:\n",
      "\n",
      "$$ 0.40y = 0.25(y + 600) $$\n",
      "\n",
      "Solving this equation:\n",
      "\n",
      "$$ 0.40y = 0.25y + 150 $$\n",
      "$$ 0.40y - 0.25y = 150 $$\n",
      "$$ 0.15y = 150 $$\n",
      "$$ y = 1000 $$\n",
      "\n",
      "So, Mrs. Snyder's previous monthly income was $1000.\n",
      "Model Answer: <answer>1000</answer>\n",
      "Expected Answer: Let her previous monthly income be p\n",
      "The cost of her rent and utilities was 40% of p which is (40/100)*p = 2p/5\n",
      "Her income was increased by $600 so it is now p+$600\n",
      "The cost of her rent and utilities now amount to 25% of (p+$600) which is (25/100)*(p+$600) = (p+$600)/4\n",
      "Equating both expressions for cost of rent and utilities: 2p/5 = (p+$600)/4\n",
      "Multiplying both sides of the equation by 20 gives 8p = 5p+$3000\n",
      "Subtracting 5p from both sides gives: 3p = $3000\n",
      "Dividing both sides by 3 gives p = $1000\n",
      "#### 1000\n",
      "\n",
      "Question 3: Read a CSV file, convert the string representations of dictionaries in a specific column ('dict_column') to Python dictionaries, and visualize the data with Seaborn's pairplot.\n",
      "The function should output with:\n",
      "    tuple: A tuple containing:\n",
      "    df (DataFrame): The DataFrame after reading and processing the CSV file.\n",
      "    ax (PairGrid): Seaborn's PairGrid object after plotting.\n",
      "You should write self-contained code starting with:\n",
      "```\n",
      "import ast\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "def task_func(csv_file):\n",
      "```\n",
      "\n",
      "Model Answer with reasoning steps: ### Step 1: Generate a similar example question along with its answer\n",
      "\n",
      "**Example Question:**  \n",
      "Read a CSV file, convert the string representations of dictionaries in a specific column ('data_column') to Python dictionaries, and visualize the data with Seaborn's pairplot. The function should output with:\n",
      "- tuple: A tuple containing:\n",
      "  - df (DataFrame): The DataFrame after reading and processing the CSV file.\n",
      "  - ax (PairGrid): Seaborn's PairGrid object after plotting.\n",
      "\n",
      "**Example Answer:**\n",
      "```python\n",
      "import ast\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "def example_func(csv_file):\n",
      "    # Read the CSV file\n",
      "    df = pd.read_csv(csv_file)\n",
      "    \n",
      "    # Convert string representations of dictionaries to actual dictionaries\n",
      "    df['data_column'] = df['data_column'].apply(ast.literal_eval)\n",
      "    \n",
      "    # Visualize the data with Seaborn's pairplot\n",
      "    ax = sns.pairplot(df)\n",
      "    \n",
      "    # Return the processed DataFrame and the PairGrid object\n",
      "    return (df, ax)\n",
      "```\n",
      "\n",
      "### Step 2: Use the example to help answer the main question\n",
      "\n",
      "Now, using the example above, I will answer the main question by writing the function `task_func(csv_file)` that reads a CSV file, converts the string representations of dictionaries in a specific column ('dict_column') to Python dictionaries, and visualizes the data with Seaborn's pairplot.\n",
      "\n",
      "### Step 3: Write the function `task_func(csv_file)`\n",
      "\n",
      "Here is the complete function:\n",
      "\n",
      "```python\n",
      "import ast\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "def task_func(csv_file):\n",
      "    # Step 1: Read the CSV file\n",
      "    df = pd.read_csv(csv_file)\n",
      "    \n",
      "    # Step 2: Convert string representations of dictionaries in 'dict_column' to actual dictionaries\n",
      "    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n",
      "    \n",
      "    # Step 3: Visualize the data with Seaborn's pairplot\n",
      "    ax = sns.pairplot(df)\n",
      "    \n",
      "    # Step 4: Return the processed DataFrame and the PairGrid object\n",
      "    return (df, ax)\n",
      "```\n",
      "\n",
      "### Final Answer:\n",
      "\n",
      "The function `task_func(csv_file)` reads a CSV file, converts the string representations of dictionaries in the 'dict_column' to actual Python dictionaries, and visualizes the data using Seaborn's pairplot. It returns a tuple containing the processed DataFrame and the PairGrid object.\n",
      "\n",
      "```python\n",
      "import ast\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "def task_func(csv_file):\n",
      "    # Step 1: Read the CSV file\n",
      "    df = pd.read_csv(csv_file)\n",
      "    \n",
      "    # Step 2: Convert string representations of dictionaries in 'dict_column' to actual dictionaries\n",
      "    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n",
      "    \n",
      "    # Step 3: Visualize the data with Seaborn's pairplot\n",
      "    ax = sns.pairplot(df)\n",
      "    \n",
      "    # Step 4: Return the processed DataFrame and the PairGrid object\n",
      "    return (df, ax)\n",
      "```\n",
      "Model Answer: <answer>\n",
      "import ast\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "\n",
      "def task_func(csv_file):\n",
      "    # Step 1: Read the CSV file\n",
      "    df = pd.read_csv(csv_file)\n",
      "    \n",
      "    # Step 2: Convert string representations of dictionaries in 'dict_column' to actual dictionaries\n",
      "    df['dict_column'] = df['dict_column'].apply(ast.literal_eval)\n",
      "    \n",
      "    # Step 3: Visualize the data with Seaborn's pairplot\n",
      "    ax = sns.pairplot(df)\n",
      "    \n",
      "    # Step 4: Return the processed DataFrame and the PairGrid object\n",
      "    return (df, ax)\n",
      "</answer>\n",
      "Expected Answer:     df = pd.read_csv(csv_file)\n",
      "    df[\"dict_column\"] = df[\"dict_column\"].apply(ast.literal_eval)\n",
      "    # Convert 'dict_column' to string representation for plotting\n",
      "    df[\"hue_column\"] = df[\"dict_column\"].apply(str)\n",
      "    ax = sns.pairplot(df, hue=\"hue_column\")\n",
      "    return df, ax\n",
      "\n",
      "Question 4: Which city does David Soul come from? Answer the question using the context.\n",
      "\n",
      " David Soul (born August 28, 1943) is an American-British actor and singer. He is known for his role as Detective Kenneth \"Hutch\" Hutchinson in the ABC television series Starsky & Hutch from 1975 to 1979. He became a British citizen in 2004. \n",
      "\n",
      "Early life\n",
      "\n",
      "Soul was born David Richard Solberg in Chicago, Illinois, on August 28, 1943. His mother, June Johnanne (Nelson), was a teacher, and his father, Dr. Richard W. Solberg, was a Lutheran minister, Professor of History and Political Science, and Director of Higher Education for the American Lutheran Church.   Dr. Solberg was also Senior Representative for Lutheran World Relief during the reconstruction of Germany after the Second World War from 1949 until 1956. Because of this, the family moved frequently while Soul was growing up. Both of his grandfathers were evangelists. \n",
      "\n",
      "Soul attended Augustana College, University of the Americas in Mexico City and the University of Minnesota. At 19, he turned down a professional baseball contract with the Chicago White Sox in order to study political science. While in Mexico, inspired by students who taught him to play the guitar, Soul changed his direction and decided to follow his passion for music. His first appearance upon returning from Mexico to the States was in a club in Minneapolis, The 10 O'Clock Scholar.\n",
      "\n",
      "Career\n",
      "\n",
      "Soul first gained attention as the \"Covered Man\" appearing on The Merv Griffin Show in 1966 and 1967, on which he sang while wearing a mask. He explained: \"My name is David Soul, and I want to be known for my music.\"  The same year, he made his television debut in Flipper.\n",
      "\n",
      "In 1967, he signed a contract with Columbia Pictures and following a number of guest appearances, including the episode \"The Apple\" from the second season of Star Trek, he landed the role of Joshua Bolt on the television program Here Come the Brides with co-stars Robert Brown, Bobby Sherman and Bridget Hanley. The series was telecast on the ABC network from September 25, 1968 to September 18, 1970.  In 1972 he co-starred as Arthur Hill's law partner on Owen Marshall: Counselor at Law. Following numerous  guest-starring roles on TV, including Streets of San Francisco, he was cast by Clint Eastwood in the film Magnum Force.\n",
      "\n",
      "His breakthrough came when he portrayed Detective Ken \"Hutch\" Hutchinson on Starsky and Hutch, a role he played from 1975 until 1979. Soul also directed three episodes of Starsky and Hutch: \"Huggy Can't Go Home\"  (1979), \"Manchild on the Streets\" (1977), and \"Survival\" (1977). Throughout his career, he has also made guest appearances on  Star Trek, I Dream of Jeannie, McMillan & Wife, Cannon, Gunsmoke, All in the Family, and numerous TV movies and mini-series including Homeward Bound (1980), World War III and Rage (1980) a TV movie commended on the floor of the US Senate and for which he received an Emmy Award nomination. Soul also starred with James Mason in the 1979 TV miniseries adaptation of Stephen King's Salem's Lot, which was also edited and released as a theatrical feature film in some countries.\n",
      "\n",
      "During the mid- to late-1970s, Soul returned to his singing roots. Produced by Tony Macaulay, he recorded hits including \"Don't Give Up on Us\" (1976) which reached No. 1 in the US and the UK, and \"Silver Lady\" (1977) which also topped the charts in the UK. From 1976 until 1978, he had five UK Top 20 singles and two Top 10 albums. From 1976 to 1982 he toured extensively in the US, Europe, Far East and South America.\n",
      "\n",
      "In the U.S., he continued to make guest appearances in various television series. He starred in \"The Manions of America\" as Caleb Staunton in 1981. He starred in the short-lived 1983 NBC series Casablanca, playing nightclub owner Rick Blaine (the immortalized role that was made famous by Humphrey Bogart in the 1942 film Casablanca), and co-starred in the NBC series The Yellow Rose during the 1983-1984 season. He also starred in the television adaptation of Ken Follett's wartime drama The Key to Rebecca (1985) directed by David Hemmings. He later starred as the infamous Florida robber Michael Platt in the TV film In the Line of Duty: The FBI Murders (1988), which depicted the 1986 FBI Miami shootout and which was subsequently used as an FBI training film. Soul also directed the episode \"No Exit\" of the 1980s TV series Miami Vice. In 1987 Soul was cast as Major Oldham in the movie The Hanoi Hilton.\n",
      "\n",
      "United Kingdom\n",
      "\n",
      "In the mid-1990s, Soul took up residence in London, United Kingdom, forging a new career on the West End stage, including the role of Chandler Tate in Comic Potential. He also participated in the successful 1997 election campaign of his personal friend Martin Bell who ran as an MP for Tatton, as well as Bell's unsuccessful campaign in Brentwood in Essex in the 2001 General Election.\n",
      " \n",
      "In 2003, Soul appeared (as himself) in the first series of the BBC's Little Britain. In 2004, he appeared in Agatha Christie's Poirot – Death on the Nile in the role of Andrew Pennington (he had also starred in the 1989 film adaptation of Christie's Appointment with Death). Soul was a guest on the BBC's Top Gear. He was one of the fastest drivers to have appeared on the show, finishing the lap in 1:54:00,  but managed to break the car's gearbox (and subsequently that of the backup car) very close to the finish.\n",
      "\n",
      "On 12 July 2004, he took over playing the role of Jerry Springer in Jerry Springer - The Opera at the Cambridge Theatre in London, which was televised by the BBC in 2005. He returned to the West End in 2006, playing Mack in a new production of Jerry Herman's musical Mack and Mabel at the Criterion Theatre. The production co-starred Janie Dee and was directed by John Doyle. He also appeared in the TV series Dalziel & Pascoe (Game of Soldiers). He had a brief cameo in the 2004 film version of Starsky & Hutch, alongside original co-star Paul Michael Glaser.\n",
      "\n",
      "In August 2008, Soul appeared in the reality TV talent show-themed television series Maestro on BBC Two. \n",
      "\n",
      "He appeared with Fred Ward and Willem Dafoe in the film Farewell directed by Christian Carion which received its US release in 2010.\n",
      "\n",
      "In June 2012, Soul made a one-week appearance with Jerry Hall at the Gaiety Theatre, Dublin in a reprise of the Pulitzer Prize nominated play by A.R. Gurney, Love Letters. \n",
      "\n",
      "On 29 July 2012, Soul appeared in an episode of the British television detective drama series Lewis, playing a murder victim. He was also featured in the hit album by Fosseytango, singing on the track \"Landlord\" (featuring Jimmy Page, on guitar).\n",
      "\n",
      "In 2013, Soul appeared in a cameo role in the Scottish film Filth lip-syncing his own recording of \"Silver Lady\".\n",
      "\n",
      "In 2014, Soul appeared in a British television commercial for National Express singing \"Silver Lady\" while driving a coach.\n",
      "\n",
      "Personal life\n",
      "\n",
      "Soul has been married five times and has five sons and a daughter. He first married the actress Mirriam \"Mim\" Solberg (née Russeth), in 1964. The couple had one child together, but the marriage only lasted a year. \n",
      "\n",
      "Soul then married actress Karen Carlson in 1968, after they had met on the set of the television series Here Come The Brides. The couple also had a child together, and divorced in 1977.\n",
      "\n",
      "During the years he was filming Starsky & Hutch, Soul had an open relationship with actress Lynne Marta.\n",
      "\n",
      "Soul's third wife was Patti Carnel Sherman (the ex-wife of fellow Here Come the Brides co-star and teen pop idol Bobby Sherman), whom he married in 1980. They had three children together, but the marriage disintegrated due to Soul's alcoholism and violent temper. Soul had been an alcoholic for several years, a problem that had affected both of his previous marriages. During his marriage to Sherman, Soul was arrested and jailed for assaulting her while she was seven months pregnant. After being released, he was ordered to attend a two-year therapy program to deal with his drinking and anger.  The couple divorced in 1986.\n",
      "\n",
      "Soul married again in 1987, to actress Julia Nickson. The couple had one child, China Soul, who is a singer/songwriter.  Soul and Nickson divorced in 1993.\n",
      "\n",
      "Soul emigrated to the United Kingdom in the mid-1990s and settled in London with his girlfriend, American actress Alexa Hamilton, though the couple later broke up.\n",
      "\n",
      "In September 2004 Soul became a British citizen while retaining dual United States citizenship. He is an avid fan of English football and is an Arsenal F.C. supporter.\n",
      "\n",
      "Soul married his fifth wife, Helen Snell, in June 2010. They had been in a relationship since 2002, after meeting when Soul was working in the British stage production of Deathtrap. \n",
      "\n",
      "Filmography\n",
      "\n",
      "Film\n",
      "\n",
      "Television\n",
      "\n",
      "Discography\n",
      "\n",
      "Albums\n",
      "\n",
      "*1976: David Soul - UK #2 Australia #8\n",
      "*1977: Playing To An Audience Of One – UK #8 Australia #30\n",
      "*1979: Band Of Friends\n",
      "*1982: The Best Days of My Life\n",
      "*1997: Leave A Light On\n",
      "Source:\n",
      "\n",
      "Singles\n",
      "\n",
      "* \"Don't Give Up On Us\" (1976) UK #1, US #1\n",
      "* \"Going In With My Eyes Open\" (1977) UK #2, US #54\n",
      "* \"Silver Lady\" (1977) UK #1, US #52\n",
      "* \"Let's Have A Quiet Night In\" (1977) UK #8\n",
      "* \"It Sure Brings Out The Love In Your Eyes\" (1978) UK #12\n",
      "Source: \n",
      "\n",
      "Bibliography\n",
      "\n",
      "* Top Pop Singles 1955-2002 by Joel Whitburn – 2003\n",
      "* The Life, The Legend by David Tailford – 1987\n",
      "\n",
      "Model Answer with reasoning steps: ### Step 1: Generate a similar example question along with its answer.\n",
      "\n",
      "**Example Question:**  \n",
      "Which city does Paul McCartney come from?  \n",
      "**Answer:**  \n",
      "Paul McCartney was born in Liverpool, England. He is one of the founding members of the Beatles and is known for his contributions to music and songwriting.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Use the example to help answer the main question.\n",
      "\n",
      "The example question asks about the birthplace of a famous person, and the answer is based on the person's early life. This is similar to the main question, which asks about the birthplace of David Soul.\n",
      "\n",
      "Now, let's analyze the context provided for David Soul:\n",
      "\n",
      "> \"David Soul (born August 28, 1943) is an American-British actor and singer. He is known for his role as Detective Kenneth \"Hutch\" Hutchinson in the ABC television series Starsky & Hutch from 1975 to 1979. He became a British citizen in 2004.\"\n",
      "\n",
      "> \"Early life  \n",
      "> Soul was born David Richard Solberg in Chicago, Illinois, on August 28, 1943.\"\n",
      "\n",
      "From this, we can see that David Soul was born in Chicago, Illinois. This is the key information needed to answer the question.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Answer:\n",
      "\n",
      "David Soul comes from Chicago, Illinois. He was born there on August 28, 1943, and his early life was marked by frequent moves due to his father's work with the American Lutheran Church.\n",
      "Model Answer: <answer>David Soul comes from Chicago, Illinois. He was born there on August 28, 1943, and his early life was marked by frequent moves due to his father's work with the American Lutheran Church.</answer>\n",
      "Expected Answer: chicago\n",
      "\n",
      "Question 5: The sequence $\\{a_n\\}$ is defined by \\[a_0 = 1,a_1 = 1, \\text{ and } a_n = a_{n - 1} + \\frac {a_{n - 1}^2}{a_{n - 2}}\\text{ for }n\\ge2.\\] The sequence $\\{b_n\\}$ is defined by \\[b_0 = 1,b_1 = 3, \\text{ and } b_n = b_{n - 1} + \\frac {b_{n - 1}^2}{b_{n - 2}}\\text{ for }n\\ge2.\\] Find $\\frac {b_{32}}{a_{32}}$ .\n",
      "\n",
      "model using python tool  def compute_sequence(initial_values, max_n):\n",
      "Model Answer with reasoning steps: To solve the problem, we need to understand the recursive definitions of the sequences $\\{a_n\\}$ and $\\{b_n\\}$, and then compute the ratio $\\frac{b_{32}}{a_{32}}$. Let's proceed step by step.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1: Understand the Recursive Definitions\n",
      "\n",
      "We are given two sequences:\n",
      "\n",
      "- Sequence $\\{a_n\\}$:\n",
      "  - $a_0 = 1$\n",
      "  - $a_1 = 1$\n",
      "  - For $n \\geq 2$, $a_n = a_{n-1} + \\frac{a_{n-1}^2}{a_{n-2}}$\n",
      "\n",
      "- Sequence $\\{b_n\\}$:\n",
      "  - $b_0 = 1$\n",
      "  - $b_1 = 3$\n",
      "  - For $n \\geq 2$, $b_n = b_{n-1} + \\frac{b_{n-1}^2}{b_{n-2}}$\n",
      "\n",
      "Both sequences are defined recursively, and the recurrence relation is similar, with the only difference being the initial conditions.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2: Generate a Similar Example Question\n",
      "\n",
      "Let’s create a similar example to help us understand the pattern and how to compute the required value.\n",
      "\n",
      "#### Example Question:\n",
      "The sequence $\\{c_n\\}$ is defined by:\n",
      "$$\n",
      "c_0 = 1, c_1 = 2, \\text{ and } c_n = c_{n-1} + \\frac{c_{n-1}^2}{c_{n-2}} \\text{ for } n \\geq 2.\n",
      "$$\n",
      "Find $\\frac{c_4}{c_2}$.\n",
      "\n",
      "#### Example Answer:\n",
      "We compute the first few terms of the sequence $\\{c_n\\}$:\n",
      "\n",
      "- $c_0 = 1$\n",
      "- $c_1 = 2$\n",
      "- $c_2 = c_1 + \\frac{c_1^2}{c_0} = 2 + \\frac{4}{1} = 6$\n",
      "- $c_3 = c_2 + \\frac{c_2^2}{c_1} = 6 + \\frac{36}{2} = 6 + 18 = 24$\n",
      "- $c_4 = c_3 + \\frac{c_3^2}{c_2} = 24 + \\frac{576}{6} = 24 + 96 = 120$\n",
      "\n",
      "Now, compute $\\frac{c_4}{c_2} = \\frac{120}{6} = 20$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 3: Use the Example to Solve the Main Question\n",
      "\n",
      "Now, we apply the same method to the original question.\n",
      "\n",
      "We are given:\n",
      "\n",
      "- $a_0 = 1$\n",
      "- $a_1 = 1$\n",
      "- $a_n = a_{n-1} + \\frac{a_{n-1}^2}{a_{n-2}}$ for $n \\geq 2$\n",
      "\n",
      "- $b_0 = 1$\n",
      "- $b_1 = 3$\n",
      "- $b_n = b_{n-1} + \\frac{b_{n-1}^2}{b_{n-2}}$ for $n \\geq 2$\n",
      "\n",
      "We need to compute $\\frac{b_{32}}{a_{32}}$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 4: Compute the First Few Terms of $\\{a_n\\}$ and $\\{b_n\\}$\n",
      "\n",
      "We will compute the first few terms of both sequences to identify any patterns or to verify the recurrence.\n",
      "\n",
      "#### Compute $\\{a_n\\}$:\n",
      "\n",
      "- $a_0 = 1$\n",
      "- $a_1 = 1$\n",
      "- $a_2 = a_1 + \\frac{a_1^2}{a_0} = 1 + \\frac{1}{1} = 2$\n",
      "- $a_3 = a_2 + \\frac{a_2^2}{a_1} = 2 + \\frac{4}{1} = 6$\n",
      "- $a_4 = a_3 + \\frac{a_3^2}{a_2} = 6 + \\frac{36}{2} = 6 + 18 = 24$\n",
      "- $a_5 = a_4 + \\frac{a_4^2}{a_3} = 24 + \\frac{576}{6} = 24 + 96 = 120$\n",
      "\n",
      "#### Compute $\\{b_n\\}$:\n",
      "\n",
      "- $b_0 = 1$\n",
      "- $b_1 = 3$\n",
      "- $b_2 = b_1 + \\frac{b_1^2}{b_0} = 3 + \\frac{9}{1} = 12$\n",
      "- $b_3 = b_2 + \\frac{b_2^2}{b_1} = 12 + \\frac{144}{3} = 12 + 48 = 60$\n",
      "- $b_4 = b_3 + \\frac{b_3^2}{b_2} = 60 + \\frac{3600}{12} = 60 + 300 = 360$\n",
      "- $b_5 = b_4 + \\frac{b_4^2}{b_3} = 360 + \\frac{129600}{60} = 360 + 2160 = 2520$\n",
      "\n",
      "---\n",
      "\n",
      "### Step 5: Observe the Pattern\n",
      "\n",
      "From the computed values, we observe that:\n",
      "\n",
      "- $\\{a_n\\}$: $1, 1, 2, 6, 24, 120, \\dots$\n",
      "- $\\{b_n\\}$: $1, 3, 12, 60, 360, 2520, \\dots$\n",
      "\n",
      "We notice that both sequences are growing very quickly, and the ratio $\\frac{b_n}{a_n}$ seems to be increasing.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 6: Use Python to Compute $\\frac{b_{32}}{a_{32}}$\n",
      "\n",
      "Given the rapid growth of the sequences, it is impractical to compute the terms manually up to $n = 32$. Instead, we can use Python to compute the terms up to $n = 32$ and then compute the ratio.\n",
      "\n",
      "Let’s write a Python script to compute the terms of both sequences up to $n = 32$ and then compute the ratio.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 7: Python Code to Compute the Sequences\n",
      "\n",
      "```python\n",
      "def compute_sequence(initial_values, max_n):\n",
      "    sequence = initial_values.copy()\n",
      "    for n in range(2, max_n + 1):\n",
      "        term = sequence[n-1] + (sequence[n-1] ** 2) / sequence[n-2]\n",
      "        sequence.append(term)\n",
      "    return sequence\n",
      "\n",
      "# Compute a_n up to n = 32\n",
      "a = compute_sequence([1, 1], 32)\n",
      "\n",
      "# Compute b_n up to n = 32\n",
      "b = compute_sequence([1, 3], 32)\n",
      "\n",
      "# Compute the ratio b_32 / a_32\n",
      "ratio = b[32] / a[32]\n",
      "ratio\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### Step 8: Final Answer\n",
      "\n",
      "After running the Python code, we find that:\n",
      "\n",
      "$$\n",
      "\\frac{b_{32}}{a_{32}} = 3^{32}\n",
      "$$\n",
      "\n",
      "This is because the sequences $\\{a_n\\}$ and $\\{b_n\\}$ are related to the factorial and the product of odd numbers, and the ratio simplifies to a power of 3.\n",
      "\n",
      "---\n",
      "\n",
      "### Final Answer:\n",
      "$$\n",
      "\\boxed{3^{32}}\n",
      "$$\n",
      "Model Answer: <answer>\n",
      "$$\n",
      "\\boxed{3^{32}}\n",
      "$$\n",
      "</answer>\n",
      "Expected Answer: 561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#calling the model to answer some of the dev questions for testing\n",
    "import random\n",
    "sample_tests = random.sample(dev_data, 5)\n",
    "# {\n",
    "#         \"input\": \"A tennis player computes her win ratio by dividing the number of matches she has won by the total number of matches she has played. At the start of a weekend, her win ratio is exactly $0.500$ . During the weekend, she plays four matches, winning three and losing one. At the end of the weekend, her win ratio is greater than $.503$ . What's the largest number of matches she could've won before the weekend began?\",\n",
    "#         \"output\": \"164\",\n",
    "#         \"domain\": \"math\"\n",
    "#     },\n",
    "\n",
    "# def call_model_chat_completions(prompt: str,\n",
    "#                                 system: str = \"You are a helpful assistant. Reply with only the final answer—no explanation.\",\n",
    "#                                 model: str = MODEL,\n",
    "#                                 temperature: float = 0.0,\n",
    "#                                 timeout: int = 60) -> dict:\n",
    "    \n",
    "#model is outputting explanation of the answer (lets stop it)\n",
    "pre_analogical_sys_prompt = \"\"\"You are a careful assistant. \n",
    "                                First think through the problem step by step before answering. \n",
    "                                Provide detail explanation of every step before providing the final answer.\"\"\"\n",
    "analogical_prompt_template = \"\"\"Follow this steps to answer the question:\n",
    "                                1. Generate a similar example question along with its answer.\n",
    "                                2. Use this example to help you answer the main question.\n",
    "                                Here are the tools at your disposal:\n",
    "                                - Google Search: useful for when you need to look up information about current events, people, places, or any other topic. Use this tool by saying \"Google Search: <your query> iexample: Google Search: who is the most handsome man in the world?\"\n",
    "                                - Python Executioner: useful for when you need to perform calculations, data analysis, or any other task that requires executing Python code. Use this tool by saying \"Python Executioner: <your python code>\" iexample: Python Executioner: print(2+2)\n",
    "                                PROVIDE YOU FINAL ANWSER WITH EVERY STEP OF YOUR REASONING.\n",
    "                                \"\"\" \n",
    "self_refine_sys_prompt =     \"\"\"You are a Meticolous grader.Your goal is to verify the correctness of answers to a given question.\"\"\"\n",
    "self_refine_prompt_temp =  \"\"\"\n",
    "                            You are a Quality Assurance Auditor. \n",
    "                            1. Check the proposed answer for logical flaws or missed constraints.\n",
    "                            2. If the answer is correct, repeat it inside <answer> tags.\n",
    "                            3. If it is wrong, fix it and output the new answer inside <answer> tags.\n",
    "                            \"\"\"\n",
    "for i, tests in enumerate(sample_tests):\n",
    "    #implementing analogical self prompting here \n",
    "    question = tests['input']\n",
    "    analogical_prompt = f\"\"\" {analogical_prompt_template}\n",
    "    Main Question: {question}\n",
    "    \"\"\"\n",
    "    # creting the payload \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": pre_analogical_sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": analogical_prompt}\n",
    "    ]\n",
    "    print(f\"Question {i+1}: {tests['input']}\\n\")\n",
    "    \n",
    "    for _ in range(7):\n",
    "        #allows for the model to use tools\n",
    "        result = call_model_chat_completions(prompt=analogical_prompt, system=pre_analogical_sys_prompt, model=MODEL, temperature=0.0, timeout=60, max_tokens=2060  , message=messages) \n",
    "        model_ans = result['text'] or  \"\" #get the answer or return an empty string\n",
    "        tool = re.search(r\"(Google Search:|Python Executioner:)(.*)\", model_ans)\n",
    "        \n",
    "        if tool:\n",
    "            tool_name = tool.group(1).strip()\n",
    "            tool_input = tool.group(2).strip()\n",
    "            if tool_name == \"Google Search:\":\n",
    "                tool_output = google_search(tool_input)\n",
    "            elif tool_name == \"Python Executioner:\":\n",
    "                tool_output = python_executioner(tool_input)\n",
    "            #update the prompt with the tool output\n",
    "            messages += [{\"role\": \"system\", \"content\": f\"Tool Output: {tool_output}\\n Continue with the main question.\"}]\n",
    "        else:\n",
    "            print(\"Model Answer with reasoning steps:\", model_ans)\n",
    "            break \n",
    "            \n",
    "    \n",
    "    #passing the modesl answer to the self refine prompt\n",
    "    self_refine_prompt = f\"\"\"{self_refine_prompt_temp}\n",
    "    Question: {question}\n",
    "    Proposed Answer: {model_ans}\n",
    "    Provide the final answer inside the <answer> tag. NO EXPlanation or fillers.\n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": self_refine_sys_prompt},\n",
    "        {\"role\": \"user\", \"content\": self_refine_prompt}\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    result = call_model_chat_completions(prompt=self_refine_prompt, model=MODEL, temperature=0.0, max_tokens=2060 , timeout=60, message=messages)\n",
    "    \n",
    "    print(\"Model Answer:\", (result[\"text\"] or \"\").strip())\n",
    "    print(\"Expected Answer:\", tests['output'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fd359",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
